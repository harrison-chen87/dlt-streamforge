{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf2216f4-3bbf-40a6-852a-312c9ea1d8ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "@dlt.table(name=\"gold.valve_compliance_history\")\n",
    "def valve_compliance_history_gold():\n",
    "    # Get current and historical valve compliance states\n",
    "    return (spark.read.<FILL_ANSWER_HERE>(\"silver.valve_compliance_changes\")\n",
    "            # Filter out NULL records in key fields\n",
    "            .filter(\"valve_id IS NOT NULL AND asset_id IS NOT NULL\")\n",
    "            # Filter out records with NULL compliance status\n",
    "            .filter(\"compliance_status IS NOT NULL\")\n",
    "            # Select and rename columns\n",
    "            .select(\n",
    "                \"valve_id\",\n",
    "                \"asset_id\",\n",
    "                \"compliance_status\",\n",
    "                \"inspector_id\",\n",
    "                \"inspection_notes\",\n",
    "                \"change_timestamp\",\n",
    "                \"__START_AT\",\n",
    "                \"__END_AT\"\n",
    "            )\n",
    "            .<FILL_ANSWER_HERE>(\"__START_AT\", \"valid_from\")\n",
    "            .<FILL_ANSWER_HERE>(\"__END_AT\", \"valid_to\")\n",
    "            # Add business insights\n",
    "            .withColumn(\"compliance_duration_days\", \n",
    "                       expr(\"datediff(valid_to, valid_from)\"))\n",
    "            .withColumn(\"is_current_record\", \n",
    "                       expr(\"CASE WHEN valid_to IS NULL THEN 'True' ELSE 'False' END\"))\n",
    "            # Deduplicate based on valve_id, asset_id, and valid_from\n",
    "            .dropDuplicates([\"valve_id\", \"asset_id\", \"valid_from\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f826d44-1559-4444-aa88-bb02c5e0f0b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, approx_count_distinct, date_trunc, col, to_date\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"gold.emissions_analytics\",\n",
    "    comment=\"Streaming gold table for emissions analytics\",\n",
    "    temporary=False\n",
    ")\n",
    "def emissions_analytics():\n",
    "    # Get base tables\n",
    "    sensor_emissions = <FILL_ANSWER_HERE>(\"silver.sensor_emissions\") \\\n",
    "        .withColumn(\"emission_date\", to_date(col(\"timestamp\")))\n",
    "    \n",
    "    site_info = <FILL_ANSWER_HERE>(\"silver.site_info\")\n",
    "    daily_weather = <FILL_ANSWER_HERE>(\"silver.daily_weather\")\n",
    "    \n",
    "    # First join - just emissions and site info\n",
    "    base_join = sensor_emissions \\\n",
    "        .join(site_info, [\"site_id\"]) \\\n",
    "        .join(daily_weather, \n",
    "              (sensor_emissions.site_id == daily_weather.site_id) & \n",
    "              (sensor_emissions.emission_date == daily_weather.date)) \\\n",
    "        .select(\n",
    "            sensor_emissions.emission_date,\n",
    "            sensor_emissions.site_id,\n",
    "            site_info.site_name,\n",
    "            daily_weather.temperature_celsius,\n",
    "            daily_weather.humidity_percentage,\n",
    "            sensor_emissions.methane_level,\n",
    "            sensor_emissions.co2_level,\n",
    "            sensor_emissions.nox_level,\n",
    "            sensor_emissions.asset_id\n",
    "        )\n",
    "    \n",
    "    # Simple aggregation\n",
    "    return base_join \\\n",
    "        .groupBy(\n",
    "            \"emission_date\",\n",
    "            \"site_id\",\n",
    "            \"site_name\",\n",
    "            \"temperature_celsius\",\n",
    "            \"humidity_percentage\"\n",
    "        ) \\\n",
    "        .<FILL_ANSWER_HERE>(\n",
    "            avg(\"methane_level\").alias(\"avg_methane_level\"),\n",
    "            avg(\"co2_level\").alias(\"avg_co2_level\"),\n",
    "            avg(\"nox_level\").alias(\"avg_nox_level\"),\n",
    "            approx_count_distinct(\"asset_id\").alias(\"approx_reporting_sensors\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d472585-042f-49ef-8ae4-e46e08cd78f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e43a565-546e-4f47-8220-8fa83251adbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "@dlt.table(name=\"gold.valve_compliance_history\")\n",
    "def valve_compliance_history_gold():\n",
    "    # Get current and historical valve compliance states\n",
    "    return (spark.read.table(\"silver.valve_compliance_changes\")\n",
    "            # Filter out NULL records in key fields\n",
    "            .filter(\"valve_id IS NOT NULL AND asset_id IS NOT NULL\")\n",
    "            # Filter out records with NULL compliance status\n",
    "            .filter(\"compliance_status IS NOT NULL\")\n",
    "            # Select and rename columns\n",
    "            .select(\n",
    "                \"valve_id\",\n",
    "                \"asset_id\",\n",
    "                \"compliance_status\",\n",
    "                \"inspector_id\",\n",
    "                \"inspection_notes\",\n",
    "                \"change_timestamp\",\n",
    "                \"__START_AT\",\n",
    "                \"__END_AT\"\n",
    "            )\n",
    "            .withColumnRenamed(\"__START_AT\", \"valid_from\")\n",
    "            .withColumnRenamed(\"__END_AT\", \"valid_to\")\n",
    "            # Add business insights\n",
    "            .withColumn(\"compliance_duration_days\", \n",
    "                       expr(\"datediff(valid_to, valid_from)\"))\n",
    "            .withColumn(\"is_current_record\", \n",
    "                       expr(\"CASE WHEN valid_to IS NULL THEN 'True' ELSE 'False' END\"))\n",
    "            # Deduplicate based on valve_id, asset_id, and valid_from\n",
    "            .dropDuplicates([\"valve_id\", \"asset_id\", \"valid_from\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4de70a47-c279-48d7-a2bb-95fca0d29b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, approx_count_distinct, date_trunc, col, to_date\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"gold.emissions_analytics\",\n",
    "    comment=\"Streaming gold table for emissions analytics\",\n",
    "    temporary=False\n",
    ")\n",
    "def emissions_analytics():\n",
    "    # Get base tables\n",
    "    sensor_emissions = dlt.read(\"silver.sensor_emissions\") \\\n",
    "        .withColumn(\"emission_date\", to_date(col(\"timestamp\")))\n",
    "    \n",
    "    site_info = dlt.read(\"silver.site_info\")\n",
    "    daily_weather = dlt.read(\"silver.daily_weather\")\n",
    "    \n",
    "    # First join - just emissions and site info\n",
    "    base_join = sensor_emissions \\\n",
    "        .join(site_info, [\"site_id\"]) \\\n",
    "        .join(daily_weather, \n",
    "              (sensor_emissions.site_id == daily_weather.site_id) & \n",
    "              (sensor_emissions.emission_date == daily_weather.date)) \\\n",
    "        .select(\n",
    "            sensor_emissions.emission_date,\n",
    "            sensor_emissions.site_id,\n",
    "            site_info.site_name,\n",
    "            daily_weather.temperature_celsius,\n",
    "            daily_weather.humidity_percentage,\n",
    "            sensor_emissions.methane_level,\n",
    "            sensor_emissions.co2_level,\n",
    "            sensor_emissions.nox_level,\n",
    "            sensor_emissions.asset_id\n",
    "        )\n",
    "    \n",
    "    # Simple aggregation\n",
    "    return base_join \\\n",
    "        .groupBy(\n",
    "            \"emission_date\",\n",
    "            \"site_id\",\n",
    "            \"site_name\",\n",
    "            \"temperature_celsius\",\n",
    "            \"humidity_percentage\"\n",
    "        ) \\\n",
    "        .agg(\n",
    "            avg(\"methane_level\").alias(\"avg_methane_level\"),\n",
    "            avg(\"co2_level\").alias(\"avg_co2_level\"),\n",
    "            avg(\"nox_level\").alias(\"avg_nox_level\"),\n",
    "            approx_count_distinct(\"asset_id\").alias(\"approx_reporting_sensors\")\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "WORKSHOP_ANSWERS",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
